{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T08:24:57.209069Z",
     "start_time": "2020-01-06T08:24:54.440425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime , date\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T08:25:06.033689Z",
     "start_time": "2020-01-06T08:24:57.218997Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"제6회 L.POINT Big Data Competition/\")\n",
    "\n",
    "online_df = pd.read_csv(\"clear_merged_online_df.csv\")\n",
    "audience = pd.read_csv(\"audience_with_group.csv\")\n",
    "base = pd.read_csv(\"normal_with_group.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T08:25:08.991647Z",
     "start_time": "2020-01-06T08:25:07.521545Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_purchase = pd.read_csv(\"buying_list_clac3.csv\")\n",
    "naver_cat_matching = pd.read_csv(\"product_table.csv\", encoding=\"euc-kr\")\n",
    "asso_df = pd.read_csv(\"clac_nm3_rules_list.csv\")\n",
    "asso_df.drop(asso_df.columns[1], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T11:56:27.645605Z",
     "start_time": "2020-01-06T11:56:27.193653Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"mappers = {\n",
    "    \"cat3_to_cat1\" : online_df[[\"clac_nm1\",\"clac_nm3\"]].dropna().drop_duplicates([\"clac_nm1\",\"clac_nm3\"], keep =\"first\").set_index(\"clac_nm3\").to_dict().get(\"clac_nm1\"),\n",
    "    \"cat1_to_naver\" : naver_cat_matching.set_index(\"clac_nm1\").to_dict().get(\"naver_cat\"),\n",
    "    \"clac_nm3\" : sorted(online_df.clac_nm3.dropna().unique().tolist())\n",
    "}\n",
    "\n",
    "with open(\"mappers.pickle\", \"wb\") as f:\n",
    "    pickle.dump(mappers, f, protocol=pickle.HIGHEST_PROTOCOL)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in audience.clnt_id:\\n    if not os.path.exists(\"./grouped_info_by_clnt_id/\"):\\n        os.mkdir(\"./grouped_info_by_clnt_id/\")\\n    tmp = audience.loc[audience.clnt_id == i].values\\n    group = tmp[0][-1].astype(int)\\n    path = \"./grouped_info_by_clnt_id/\"\\n    file = \"grouped_clnt_info_{}.pickle\".format(i)\\n    with open(os.path.join(path,file), \"wb\") as f:\\n        pickle.dump(tmp, f, protocol=pickle.HIGHEST_PROTOCOL)\\n\\nfor i in base.clnt_id:\\n    if not os.path.exists(\"./grouped_info_by_clnt_id/\"):\\n        os.mkdir(\"./grouped_info_by_clnt_id/\")\\n    tmp = base.loc[base.clnt_id == i].values\\n    group = tmp[0][-1].astype(int)\\n    path = \"./grouped_info_by_clnt_id/\"\\n    file = \"grouped_clnt_info_{}.pickle\".format(i)\\n    with open(os.path.join(path,file), \"wb\") as f:\\n        pickle.dump(tmp, f, protocol=pickle.HIGHEST_PROTOCOL)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존고객들 정보를 clnt_id별로 나누어 array형태 pickle파일로 저장\n",
    "# 이후에 정보를 불러올때 전체파일을 불러오면 시간상 손해가 많아 각각 나눠놓음\n",
    "\"\"\"\n",
    "for i in audience.clnt_id:\n",
    "    if not os.path.exists(\"./grouped_info_by_clnt_id/\"):\n",
    "        os.mkdir(\"./grouped_info_by_clnt_id/\")\n",
    "    tmp = audience.loc[audience.clnt_id == i].values\n",
    "    group = tmp[0][-1].astype(int)\n",
    "    path = \"./grouped_info_by_clnt_id/\"\n",
    "    file = \"grouped_clnt_info_{}.pickle\".format(i)\n",
    "    with open(os.path.join(path,file), \"wb\") as f:\n",
    "        pickle.dump(tmp, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "for i in base.clnt_id:\n",
    "    if not os.path.exists(\"./grouped_info_by_clnt_id/\"):\n",
    "        os.mkdir(\"./grouped_info_by_clnt_id/\")\n",
    "    tmp = base.loc[base.clnt_id == i].values\n",
    "    group = tmp[0][-1].astype(int)\n",
    "    path = \"./grouped_info_by_clnt_id/\"\n",
    "    file = \"grouped_clnt_info_{}.pickle\".format(i)\n",
    "    with open(os.path.join(path,file), \"wb\") as f:\n",
    "        pickle.dump(tmp, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "with open(\"holiday_list.pickle\", \"rb\") as f:\n",
    "    holiday_list = pickle.load(f)\n",
    "    \n",
    "with open(\"naver_shopping_insight.pickle\", \"rb\") as f:\n",
    "    naver_shopping_insight = pickle.load(f)\n",
    "\n",
    "with open(\"lstm_trend_dict.pickle\", \"rb\") as f:\n",
    "    trend_dict = pickle.load(f)\n",
    "    \n",
    "with open(\"clac_nm_list.pickle\", 'rb') as f:\n",
    "    clac_nm_list = pickle.load(f)\n",
    "    \n",
    "dnn = tf.keras.models.load_model(\"dnn_model.h5\")\n",
    "\n",
    "#sorted_cat = sorted(naver_shopping_insight.items(), key=(lambda x : x[1]), reverse=True)\n",
    "\n",
    "cat3_to_cat1 = online_df[[\"clac_nm1\",\"clac_nm3\"]].dropna().drop_duplicates([\"clac_nm1\",\"clac_nm3\"], keep =\"first\").set_index(\"clac_nm3\").to_dict().get(\"clac_nm1\")\n",
    "cat1_to_naver = naver_cat_matching.set_index(\"clac_nm1\").to_dict().get(\"naver_cat\")\n",
    "clac_nm3 = sorted(online_df.clac_nm3.dropna().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buying_list = buying_list.iloc[~buying_list.apply(lambda x : x[1:].sum() == x.iloc[-1], axis=1).values]\n",
    "#buying_list = buying_list.iloc[:,0:-1]\n",
    "#buying_list.to_csv(\"buying_list_clac3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Naver Shopping Insight\n",
    "\n",
    "client_id = \"aDa9tsBBqchLu4JZ6awC\"\n",
    "client_secret = \"XP6uLmitVE\"\n",
    "\n",
    "def getTrend(date=\"2019-10-01\"):\n",
    "    body1 = '{\"startDate\":\"'+str(date)+'\",\"endDate\":\"'+str(date)+'\",\"timeUnit\":\"date\",\"category\":[{\"name\":\"패션의류\",\"param\":[\"50000000\"]},{\"name\":\"패션잡화\",\"param\":[\"50000001\"]},{\"name\":\"화장품/미용\",\"param\":[\"50000002\"]}]}'\n",
    "    body2 = '{\"startDate\":\"'+str(date)+'\",\"endDate\":\"'+str(date)+'\",\"timeUnit\":\"date\",\"category\":[{\"name\":\"디지털/가전\",\"param\":[\"50000003\"]},{\"name\":\"가구/인테리어\",\"param\":[\"50000004\"]},{\"name\":\"출산/육아\",\"param\":[\"50000005\"]}]}'\n",
    "    body3 = '{\"startDate\":\"'+str(date)+'\",\"endDate\":\"'+str(date)+'\",\"timeUnit\":\"date\",\"category\":[{\"name\":\"식품\",\"param\":[\"50000006\"]},{\"name\":\"스포츠/레저\",\"param\":[\"50000007\"]},{\"name\":\"생활/건강\",\"param\":[\"50000008\"]}]}'\n",
    "    body4 = '{\"startDate\":\"'+str(date)+'\",\"endDate\":\"'+str(date)+'\",\"timeUnit\":\"date\",\"category\":[{\"name\":\"패션의류\",\"param\":[\"50000000\"]},{\"name\":\"디지털/가전\",\"param\":[\"50000003\"]},{\"name\":\"생활/건강\",\"param\":[\"50000008\"]}]}'\n",
    "    \n",
    "    url = \"https://openapi.naver.com/v1/datalab/shopping/categories\"\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    request.add_header(\"Content-Type\",\"application/json\")\n",
    "    \n",
    "    response1 = urllib.request.urlopen(request, data=body1.encode(\"utf-8\"))\n",
    "    response2 = urllib.request.urlopen(request, data=body2.encode(\"utf-8\"))\n",
    "    response3 = urllib.request.urlopen(request, data=body3.encode(\"utf-8\"))\n",
    "    response4 = urllib.request.urlopen(request, data=body4.encode(\"utf-8\"))\n",
    "    response_list = [response1, response2, response3]\n",
    "    \n",
    "    # 각 body 별 비율 계산\n",
    "    body_rate = []\n",
    "    rescode = response4.getcode()\n",
    "    response_body = json.loads(response4.read().decode('utf-8'))\n",
    "    for i in range(0,3):\n",
    "        body_rate.append(response_body[\"results\"][i][\"data\"][0][\"ratio\"])\n",
    "    body_rate = [i/max(body_rate) for i in body_rate]\n",
    "    \n",
    "    search_rate_dict = {}\n",
    "    num=0\n",
    "    for res in response_list:\n",
    "        rescode = res.getcode()\n",
    "        if (rescode==200):\n",
    "            response_body = res.read().decode('utf-8')\n",
    "            response_body = json.loads(response_body)\n",
    "            rate = body_rate[num]\n",
    "            for i in range(0,3):\n",
    "                search_rate = response_body[\"results\"][i][\"data\"][0][\"ratio\"]*rate\n",
    "                search_rate_dict[response_body[\"results\"][i][\"title\"]] = search_rate\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "        num+=1\n",
    "    search_rate_dict[\"기타\"]=0\n",
    "    \n",
    "    #print(response_result)\n",
    "    return search_rate_dict\n",
    "\n",
    "search_rate_dict = getTrend()\n",
    "\n",
    "with open(\"naver_shopping_insight.pickle\", \"wb\") as f:\n",
    "    pickle.dump(search_rate_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "naver_shopping_insight = getTrend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,7):\n",
    "    globals()[\"g{}_weight\".format(i)] = pd.DataFrame({\"col\" : col, \"weight\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T03:37:00.186831Z",
     "start_time": "2020-01-06T03:37:00.178833Z"
    }
   },
   "outputs": [],
   "source": [
    "g1_weight = g2_weight = g3_weight = g4_weight = g5_weight = g6_weight = pd.DataFrame({\"col\" : col, \"weight\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T03:36:52.788467Z",
     "start_time": "2020-01-06T03:36:52.784470Z"
    }
   },
   "outputs": [],
   "source": [
    "del[g1_weight,g2_weight,g3_weight,g4_weight,g5_weight,g6_weight ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T03:36:09.979538Z",
     "start_time": "2020-01-06T03:36:09.959767Z"
    }
   },
   "outputs": [],
   "source": [
    "col = list(audience.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using KNN\n",
    "\"\"\"group1: time_to_last, last_access\n",
    "group2: return_rate, src_PUSH\n",
    "group3: action_0, action_2, return_rate\n",
    "group4: tot_sess_cnt, pag_view_mean, src_WEBSITE\n",
    "group5: tot_sess_cnt, return_rate, action_0\n",
    "group6: action_2, tot_sess_cnt\"\"\"\n",
    "\n",
    "# 각 그룹별 weight 수정 예정\n",
    "# 각 normal 그룹에서 구매율 class label로 해서 중요변수 도출?\n",
    "col = list(audience.columns[1:-1])\n",
    "g1_weight = pd.DataFrame({\"col\" : col, \"weight\":1})\n",
    "g2_weight = pd.DataFrame({\"col\" : col, \"weight\":1})\n",
    "g3_weight = pd.DataFrame({\"col\" : col, \"weight\":1})\n",
    "g4_weight = pd.DataFrame({\"col\" : col, \"weight\":1})\n",
    "g5_weight = pd.DataFrame({\"col\" : col, \"weight\":1})\n",
    "g6_weight = pd.DataFrame({\"col\" : col, \"weight\":1})\n",
    "g1_weight.iloc[[col.index(\"time_to_last\"), col.index(\"last_access\")], 1] = 0.8\n",
    "g2_weight.iloc[[col.index(\"return_rate\"), col.index(\"src_PUSH\")], 1] = 0.8\n",
    "g3_weight.iloc[[col.index(\"action_0\"), col.index(\"action_2\"), col.index(\"return_rate\")], 1] = 0.8\n",
    "g4_weight.iloc[[col.index(\"tot_sess_cnt\"), col.index(\"pag_view_mean\"), col.index(\"src_WEBSITE\")], 1] = 0.8\n",
    "g5_weight.iloc[[col.index(\"tot_sess_cnt\"), col.index(\"return_rate\"), col.index(\"action_0\")], 1] = 0.8\n",
    "g6_weight.iloc[[col.index(\"action_2\"), col.index(\"tot_sess_cnt\")], 1] = 0.8\n",
    "\n",
    "weight_list_by_group = {1: list(g1_weight.weight),\n",
    "                       2: list(g2_weight.weight),\n",
    "                       3: list(g3_weight.weight),\n",
    "                       4: list(g4_weight.weight),\n",
    "                       5: list(g5_weight.weight),\n",
    "                       6: list(g6_weight.weight),}\n",
    "\n",
    "def similar_user(user_info, user_group, n_neighbor):\n",
    "    # user_info, user_group => 함수내에서 자동으로 input\n",
    "    \n",
    "    weight_list = weight_list_by_group.get(user_group)\n",
    "    calc_dist = base.iloc[:,1:-1].apply(lambda x :np.linalg.norm(np.multiply(weight_list,(x -  user_info))), axis=1).reset_index(name = \"dist\")\n",
    "    top_30_similarity = calc_dist.sort_values(by = \"dist\").iloc[:n_neighbor,0]\n",
    "    return base.iloc[top_30_similarity,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = base[base[\"clnt_id\"].isin(list(dataset_purchase.clnt_id))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_user(user_info, user_group, n_neighbor):\n",
    "    # user_info, user_group => 함수내에서 자동으로 input\n",
    "    \n",
    "    weight_list = weight_list_by_group.get(user_group)\n",
    "    calc_dist = base.iloc[:,1:-1].apply(lambda x :np.linalg.norm(np.multiply(weight_list,(x -  user_info))), axis=1).reset_index(name = \"dist\")\n",
    "    top_30_clnt = calc_dist.sort_values(by = \"dist\").iloc[:n_neighbor,0]\n",
    "    top_30_similarity = calc_dist.sort_values(by = \"dist\").iloc[:n_neighbor,1]\n",
    "    top_30 = pd.DataFrame({\"clnt_id\": base.iloc[top_30_clnt,0], \"dist\": top_30_similarity}).reset_index(drop=True)\n",
    "    return top_30\n",
    "\n",
    "def multiplier(weight, array):\n",
    "    return weight*array\n",
    "\n",
    "def similar_user_weight(top_30):\n",
    "    # dataset_purchase에 붙여서 가중치 계산\n",
    "    dataset_purchase_top = dataset_purchase[dataset_purchase[\"clnt_id\"].isin(list(top_30.clnt_id))].reset_index(drop=True)\n",
    "    top_30[\"weight\"] = 1 / (top_30.dist+(max(list(top_30.dist))-min(list(top_30.dist)))*2)\n",
    "    top_30 = top_30.drop(\"dist\", axis=1)\n",
    "    top_30_df = pd.merge(top_30, dataset_purchase_top, on=\"clnt_id\", how=\"left\")\n",
    "    top_30_df = top_30_df.apply(lambda x : multiplier(x['weight'], x[2:]),axis=1)\n",
    "    weight_knn = top_30_df.sum(axis=0)\n",
    "    return weight_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clac_nm_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-bdfb6c8360a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0msales_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_sales_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_trend_by_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mtrend_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"clac_nm1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mclac_nm_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sales\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msales_diff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msales_diff\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"clac_nm1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sales\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clac_nm_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Using LSTM \n",
    "\n",
    "def get_recent_sales():\n",
    "    with open(\"sales_by_day.pickle\", \"rb\") as f:\n",
    "        sales = pickle.load(f)\n",
    "    recent_sales = pd.DataFrame(sales).iloc[:,-7:]\n",
    "    return recent_sales\n",
    "\n",
    "def get_recent_time_info(start_time = '20190924', now_time = '20191001'):\n",
    "    time_info = pd.DataFrame({\"time\" : pd.date_range(start=start_time, end=now_time)})\n",
    "\n",
    "    time_info[\"is_holiday\"] = np.where(time_info.isin(pd.to_datetime(holiday_list)), 1, 0)\n",
    "    time_info[\"is_weekend\"] = np.where(time_info.time.dt.weekday.isin(range(0,5)), 0, 1)\n",
    "\n",
    "    time_info[\"holiday_tomorrow\"] = time_info.is_holiday.shift(-1, fill_value=0)\n",
    "    time_info[\"weekend_tomorrow\"] = time_info.is_weekend.shift(-1, fill_value=0)\n",
    "\n",
    "    time_info.drop([\"is_holiday\", \"is_weekend\"], axis=1, inplace=True)\n",
    "    \n",
    "    return time_info\n",
    "\n",
    "def predict_trend_by_lstm():\n",
    "    import gc\n",
    "    with open(\"clac_nm_list.pickle\", 'rb') as f:\n",
    "        clac_nm_list = pickle.load(f)\n",
    "        \n",
    "    recent_sales = get_recent_sales()\n",
    "    time_info = get_recent_time_info()\n",
    "    yesterday_sales = get_recent_sales().iloc[:,-1].values\n",
    "    expected_sales_list = []\n",
    "    \n",
    "    for i in range(recent_sales.shape[0]):\n",
    "        # print(\"Expecting Today's {} Sales \".format(clac_nm_list[i]))\n",
    "        sales = pd.concat([recent_sales.iloc[i:i+1].T.reset_index(drop = True), time_info.iloc[:-1,1:]], axis=1).values.reshape(1,7,3)\n",
    "        modelname = os.curdir + \"/models/lstm_model_{}.h5\".format(clac_nm_list[i].replace(\"/\",\"_\"))\n",
    "        model = tf.keras.models.load_model(modelname)\n",
    "        expected_sales = np.round(model.predict(sales)[0][0])\n",
    "        expected_sales_list.append(expected_sales)\n",
    "        \n",
    "    sales_diff = yesterday_sales - np.array(expected_sales_list)\n",
    "    \n",
    "    del [recent_sales, time_info]\n",
    "    gc.collect()\n",
    "    \n",
    "    return sales_diff, expected_sales_list\n",
    "\n",
    "sales_diff, expected_sales_list = predict_trend_by_lstm()\n",
    "trend_dict = pd.DataFrame({\"clac_nm1\":clac_nm_list, \"sales\":np.where(sales_diff > 0, 1.1, np.where(sales_diff==0, 1, 0.9))}).set_index(\"clac_nm1\").to_dict().get(\"sales\")\n",
    "\n",
    "with open(\"lstm_trend_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(trend_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T11:50:54.905729Z",
     "start_time": "2020-01-06T11:50:54.888773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Association Rule Discovery\n",
    "def split_item(item_in_brace):\n",
    "    return item_in_brace.strip(\"{}\").split(\",\")\n",
    "\n",
    "asso_df['lhs'] = asso_df.lhs.apply(lambda x : split_item(x))\n",
    "asso_df['rhs'] = asso_df.rhs.apply(lambda x : split_item(x))\n",
    "asso_df['lhs_length'] = asso_df.lhs.apply(lambda x : len(x))\n",
    "\n",
    "asso_df = asso_df.sort_values(by = [\"lhs_length\",\"confidence\"], ascending = [1,0]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T11:51:19.709073Z",
     "start_time": "2020-01-06T11:51:19.692073Z"
    }
   },
   "outputs": [],
   "source": [
    "asso_df.to_csv(\"association_rule_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연관된 추천상품 찾기\n",
    "def find_asso(recommend):\n",
    "    if (type(recommend)!=list):\n",
    "        recommend = recommend.to_list()\n",
    "    asso_list = []\n",
    "    asso_conf = []\n",
    "    for i in range(len(recommend)):\n",
    "        for j in range(len(asso_df[asso_df.lhs_length==1])):\n",
    "            if (recommend[i] == asso_df.lhs[j][0]):\n",
    "                asso_list.append(asso_df.rhs[j][0])\n",
    "                asso_conf.append(asso_df.confidence[j])\n",
    "    recommend_permu = list(map(','.join, itertools.permutations(recommend, 2)))\n",
    "    for i in range(len(recommend_permu)):\n",
    "        for j in range(len(asso_df[asso_df.lhs_length>1])):\n",
    "            if (recommend_permu[i].split(',')[0] in asso_df.lhs[j] and recommend_permu[i].split(',')[1] in asso_df.lhs[j]):\n",
    "                asso_list.append(asso_df.rhs[j][0])\n",
    "                asso_conf.append(asso_df.confidence[j])\n",
    "    asso_result = pd.DataFrame({\"product\": asso_list, \"confidence\": asso_conf}).sort_values(by=\"confidence\", ascending=False)\n",
    "    asso_result = asso_result.drop_duplicates(['product'], keep='first').reset_index(drop=True)\n",
    "    return asso_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online_df[online_df.action_type == 6].clac_nm1.map(cat1_to_naver).value_counts()\n",
    "\n",
    "#naver_shopping_insight\n",
    "\n",
    "def naver_scaler(x):\n",
    "    return 1/200*x + 1/2\n",
    "\n",
    "def get_user_info(clnt_id):\n",
    "    try:\n",
    "        path = \"./grouped_info_by_clnt_id/\"\n",
    "        file = \"grouped_clnt_info_{}.pickle\".format(clnt_id)\n",
    "        with open(os.path.join(path,file), \"rb\") as f:\n",
    "            user_log_summary = pickle.load(f)\n",
    "        \n",
    "        return user_log_summary\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "def recommender(clnt_id, n_neighbor = 30):\n",
    "    user_log_summary = get_user_info(clnt_id)\n",
    "    weight = pd.Series([0]*len(clac_nm3))\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "\n",
    "    #-------------------- 고객정보가 필요하지 않은 부분 ---------------\n",
    "    #1. 네이버 API를 통해 쇼핑인사이트의 각 카테고리별 상대적 검색비중을 확인\n",
    "    #naver_search_cat = [x[0] for x in sorted(naver_shopping_insight.items(), key=(lambda x : x[1]), reverse=True)]\n",
    "    #naver_search_val = [x[1] for x in sorted(naver_shopping_insight.items(), key=(lambda x : x[1]), reverse=True)]\n",
    "    print(\"1) Naver Trend\", end=\"\")\n",
    "    weight_naver = pd.Series(clac_nm3).map(cat3_to_cat1).map(cat1_to_naver).map(naver_shopping_insight)\n",
    "    weight_naver_scaling = naver_scaler(weight_naver)\n",
    "    print(\"\\t...Success!\")\n",
    "    \n",
    "    #2. 대분류 lstm\n",
    "    print(\"2) LSTM\", end=\"\")\n",
    "    weight_lstm = pd.Series(clac_nm3).map(cat3_to_cat1).map(trend_dict)\n",
    "    print(\"\\t...Success!\")\n",
    "    \n",
    "    #-------------------- 고객정보 필요한 부분 ------------------------\n",
    "    #3. knn - 비슷한 사람 - 소분류 가중치\n",
    "    print(\"3) KNN\", end=\"\")\n",
    "    user_info = user_log_summary[0][1:-1]\n",
    "    user_group = user_log_summary[0][-1].astype(int)\n",
    "    weight_knn = similar_user_weight(similar_user(user_info, user_group, n_neighbor))\n",
    "    weight_knn_scaling = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(weight_knn)))[0]\n",
    "    print(\"\\t...Success!\")\n",
    "        \n",
    "    #4. dnn - 행동패턴을 통해 구매할 소분류 예측\n",
    "    print(\"4) DNN\", end=\"\")\n",
    "    weight_dnn = pd.Series(dnn.predict(np.expand_dims(list(user_info), axis=0))[0])\n",
    "    weight_dnn_scaling = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(weight_dnn)))[0]\n",
    "    print(\"\\t...Success!\")\n",
    "    \n",
    "    # 최종 가중치 계산 및 추천상품 선정\n",
    "    final_weight = weight_lstm*(weight_naver_scaling + weight_knn_scaling + weight_dnn_scaling)\n",
    "    final_result_df = pd.DataFrame({\"product\": clac_nm3, \"weight\": final_weight}).sort_values(by=\"weight\", ascending=False)\n",
    "    final_result = final_result_df.iloc[0:9,0].reset_index(drop=True)\n",
    "    \n",
    "    # 5. Association Rule Discovery - 추천된 상품과 함께사면 좋을 음식 추천\n",
    "    # 음식 아닌 상품 + 음식 + 함께사면 좋을 음식\n",
    "    #not_food = final_result[final_result.map(cat3_to_cat1).map(cat1_to_naver)!=\"식품\"]\n",
    "    asso_list = find_asso(final_result)[\"product\"]\n",
    "    for prod in asso_list:\n",
    "        if (prod not in list(final_result)):\n",
    "            final_result[9] = prod\n",
    "            break\n",
    "    if (len(final_result)==9):\n",
    "        final_result[9] = final_result_df.iloc[10,0]\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Naver Trend\t...Success!\n",
      "2) LSTM\t...Success!\n",
      "3) KNN\t...Success!\n",
      "4) DNN\t...Success!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                  Chicken Eggs\n",
       "1                                        Ramens\n",
       "2    Other General Stationary / Office Supplies\n",
       "3                                General Snacks\n",
       "4                          Women's Trench Coats\n",
       "5                                     Necklaces\n",
       "6                                    Fresh Milk\n",
       "7                               Boy's Sleepwear\n",
       "8                  Women's Golf T-shirts / Tops\n",
       "9                                   Corn Snacks\n",
       "Name: product, dtype: object"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
